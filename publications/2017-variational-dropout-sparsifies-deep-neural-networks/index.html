<!DOCTYPE html>
<html lang="en">
<head>
        
        <title>Variational Dropout Sparsifies Deep Neural Networks – Bayesian Methods Research Group</title>
        <meta property="twitter:site" content="@bayesgroup" />
        <meta property="twitter:card" content="summary" />
        <meta property="og:title" content="Variational Dropout Sparsifies Deep Neural Networks – Bayesian Methods Research Group" />
        <meta property="og:description" content="We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the..." />
        <meta property="og:image" content="http://bayesgroup.ru/theme/images/bayesgroup_opengraph.png" />
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <!--[if lte IE 8]><script src="/theme/js/ie/html5shiv.js"></script><![endif]-->
        <link rel="stylesheet" href="/theme/css/main.css" />
        <!--[if lte IE 8]><link rel="stylesheet" href="/theme/css/ie8.css" /><![endif]-->
</head>

<body>
        <!-- Header -->
            <section id="header">
                <header>
                    <span class="image"><a href="/"><img src="/theme/images/bayesgroup.png" alt="" /></a></span>
                    <h1 id="logo">Bayesian Methods Research Group</h1>
                    <p>Machine Learning Research Group from Moscow, Russia</p>
                </header>
                <nav id="nav">
                    <ul>
                                    <li><a href="/about/">About</a></li>
                                    <li><a href="/people">People</a></li>
                                    <li class="active"><a href="/publications">Publications</a></li>
                                    <li><a href="/teaching">Teaching</a></li>
                                    <li><a href="/admission/">Admission</a></li>
                                    <li><a href="/alumni/">Alumni</a></li>
                    </ul>
                </nav>
                <footer>
                    <ul class="icons">
                        <li><a href="https://twitter.com/bayesgroup" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
                        <li><a href="https://www.youtube.com/channel/UC9KcwaZ9gSvcYNs7Jx3oNaQ" class="icon fa-youtube"><span class="label">Youtube</span></a></li>
                        <li><a href="mailto:info@bayesgroup.ru" class="icon fa-envelope"><span class="label">Email</span></a></li>
                    </ul>
                </footer>
            </section>

        <!-- Wrapper -->
            <div id="wrapper">

                <!-- Group Photo -->
                    <section id="photo">
                        <div class="container">
                            <span class="image fit"><img src="/theme/images/banner.jpg" alt="" /></span>
                         </div>
                    </section>

                <!-- Main -->
                    <div id="main">
<section id="page">
	<div class="container">
		<header class="major">
			<h2>Variational Dropout Sparsifies Deep Neural Networks</h2>
		</header>
    

    <p>We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic
relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.</p>
<div class="video-wrapper ratio-16-9">
<iframe width="560" height="315" src="https://www.youtube.com/embed/CZ3fPKlHDoM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>

<p>The code is on <a href="https://github.com/ars-ashuha/variational-dropout-sparsifies-dnn">Github</a> and the paper is on <a href="https://arxiv.org/abs/1701.05369">arXiv</a>.</p>

    </div>
</section>
                    </div>

                <!-- Footer -->
                    <section id="footer">
                        <div class="container">
                            <ul class="copyright">
                                <li>Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
                            </ul>
                        </div>
                    </section>

            </div>

        <!-- Scripts -->
            <script src="/theme/js/jquery.min.js"></script>
            <script src="/theme/js/jquery.scrollzer.min.js"></script>
            <script src="/theme/js/jquery.scrolly.min.js"></script>
            <script src="/theme/js/skel.min.js"></script>
            <script src="/theme/js/util.js"></script>
            <!--[if lte IE 8]><script src="theme/js/ie/respond.min.js"></script><![endif]-->
            <script src="/theme/js/main.js"></script>

</body>
</html>